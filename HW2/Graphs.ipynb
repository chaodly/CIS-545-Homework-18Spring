{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS 545 Homework 2: Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: decorator>=4.1.0 in /opt/conda/lib/python3.6/site-packages (from networkx)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Execute this once, the first time you run\n",
    "!pip install networkx\n",
    "\n",
    "# Disable Python warning messages - you should probably only run this before submission\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1 Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Connect to Spark as per Step 2.1\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName('Graphs-HW2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|from_node|to_node|\n",
      "+---------+-------+\n",
      "|      123|    456|\n",
      "|      456|    789|\n",
      "|      456|    890|\n",
      "+---------+-------+\n",
      "\n",
      "+---------+-------+\n",
      "|from_node|to_node|\n",
      "+---------+-------+\n",
      "|      123|    456|\n",
      "+---------+-------+\n",
      "\n",
      "+---------+-------+\n",
      "|from_node|to_node|\n",
      "+---------+-------+\n",
      "|      123|    456|\n",
      "+---------+-------+\n",
      "\n",
      "+---------+-------+\n",
      "|from_node|to_node|\n",
      "+---------+-------+\n",
      "|      123|    456|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load some dummy data, which should be overwritten in Step 2.2\n",
    "\n",
    "answers_sdf = spark.createDataFrame([{'from_node': 123, 'to_node': 456},\\\n",
    "                                    {'from_node': 456, 'to_node': 789},\n",
    "                                    {'from_node': 456, 'to_node': 890}])\n",
    "comments_answers_sdf = spark.createDataFrame([{'from_node': 123, 'to_node': 456}])\n",
    "comments_questions_sdf = spark.createDataFrame([{'from_node': 123, 'to_node': 456}])\n",
    "\n",
    "graph_sdf = spark.createDataFrame([{'from_node': 123, 'to_node': 456}])\n",
    "\n",
    "answers_sdf.show()\n",
    "comments_answers_sdf.show()\n",
    "comments_questions_sdf.show()\n",
    "graph_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2 Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|           value|\n",
      "+----------------+\n",
      "|  9 8 1217567877|\n",
      "|  1 1 1217573801|\n",
      "| 13 1 1217606247|\n",
      "| 17 1 1217617639|\n",
      "| 48 2 1217618182|\n",
      "| 17 1 1217618239|\n",
      "| 19 9 1217618357|\n",
      "|13 23 1217618560|\n",
      "|13 11 1217618799|\n",
      "|23 23 1217619360|\n",
      "|35 33 1217620542|\n",
      "|39 33 1217620597|\n",
      "|43 22 1217620971|\n",
      "|17 32 1217621272|\n",
      "|39 40 1217621416|\n",
      "|37 40 1217621670|\n",
      "|45 45 1217621917|\n",
      "|17 17 1217622124|\n",
      "|49 13 1217623079|\n",
      "|13 23 1217623216|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|     1 91 1220713630|\n",
      "|     3 91 1220713792|\n",
      "|  380 350 1220715736|\n",
      "|4642 2257 1220734307|\n",
      "|4642 1324220 1220...|\n",
      "|2495 4285 1220736640|\n",
      "|4642 4893 1220737355|\n",
      "|2515 4903 1220738560|\n",
      "|2515 4893 1220739071|\n",
      "|  199 199 1220741079|\n",
      "|  658 658 1220742035|\n",
      "| 292 1694 1220744791|\n",
      "|4213 4213 1220747610|\n",
      "|4213 1694 1220747855|\n",
      "|2353001 825 12207...|\n",
      "|4064 3666 1220754304|\n",
      "| 4064 828 1220755251|\n",
      "|4906 3205 1220757270|\n",
      "|3394 1569 1220757715|\n",
      "|4906 4903 1220758390|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|4550 4550 1220729190|\n",
      "|  242 184 1220733503|\n",
      "|4213 4946 1220768149|\n",
      "|    91 91 1220768295|\n",
      "|2658 1874 1220771891|\n",
      "|4035 1874 1220773037|\n",
      "|2257 4489 1220802041|\n",
      "|  577 577 1220834891|\n",
      "|4489 4489 1220853536|\n",
      "| 828 2783 1220854143|\n",
      "|1412 5113 1220857132|\n",
      "|3446 5113 1220862643|\n",
      "|3446 5113 1220862734|\n",
      "|1790 5119 1220867837|\n",
      "|1412 5113 1220871353|\n",
      "|4489 1360 1220876561|\n",
      "|2653 4906 1220879177|\n",
      "|4491 4491 1220882092|\n",
      "|1772 4903 1220886524|\n",
      "| 952 3196 1220886793|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: load data as per Step 2.2\n",
    "\n",
    "answers_sdf = spark.read.load('sx-stackoverflow-a2q.txt', format = \"text\")\n",
    "comments_answers_sdf = spark.read.load('sx-stackoverflow-c2a.txt', format = \"text\")\n",
    "comments_questions_sdf = spark.read.load('sx-stackoverflow-c2q.txt', format = \"text\")\n",
    "\n",
    "answers_sdf.show()\n",
    "comments_answers_sdf.show()\n",
    "comments_questions_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17823525"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|           value|\n",
      "+----------------+\n",
      "|  9 8 1217567877|\n",
      "|  1 1 1217573801|\n",
      "| 13 1 1217606247|\n",
      "| 17 1 1217617639|\n",
      "| 48 2 1217618182|\n",
      "| 17 1 1217618239|\n",
      "| 19 9 1217618357|\n",
      "|13 23 1217618560|\n",
      "|13 11 1217618799|\n",
      "|23 23 1217619360|\n",
      "|35 33 1217620542|\n",
      "|39 33 1217620597|\n",
      "|43 22 1217620971|\n",
      "|17 32 1217621272|\n",
      "|39 40 1217621416|\n",
      "|37 40 1217621670|\n",
      "|45 45 1217621917|\n",
      "|17 17 1217622124|\n",
      "|49 13 1217623079|\n",
      "|13 23 1217623216|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25405374"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_answers_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|     1 91 1220713630|\n",
      "|     3 91 1220713792|\n",
      "|  380 350 1220715736|\n",
      "|4642 2257 1220734307|\n",
      "|4642 1324220 1220...|\n",
      "|2495 4285 1220736640|\n",
      "|4642 4893 1220737355|\n",
      "|2515 4903 1220738560|\n",
      "|2515 4893 1220739071|\n",
      "|  199 199 1220741079|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_answers_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_answers_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20268151"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_questions_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|4550 4550 1220729190|\n",
      "|  242 184 1220733503|\n",
      "|4213 4946 1220768149|\n",
      "|    91 91 1220768295|\n",
      "|2658 1874 1220771891|\n",
      "|4035 1874 1220773037|\n",
      "|2257 4489 1220802041|\n",
      "|  577 577 1220834891|\n",
      "|4489 4489 1220853536|\n",
      "| 828 2783 1220854143|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_questions_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_questions_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2b804a9b19fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0manswers_sdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'select split (value, \" \") as from_node, split (value, \" \") as to_node, split (value, \" \") as edge_type from answers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0manswers_sdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswers_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswers_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from_node\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswers_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"to_node\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswers_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"edge_type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0manswers_sdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswers_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'edge_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'answers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1020\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1021\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "# TODO: wrangling work in Step 2.3.  Add as many Cells as you need\n",
    "\n",
    "answers_sdf.createOrReplaceTempView('answers')\n",
    "answers_sdf = spark.sql('select split (value, \" \") as from_node, split (value, \" \") as to_node, split (value, \" \") as edge_type from answers')\n",
    "\n",
    "answers_sdf = answers_sdf.select(F.split(answers_sdf.value, \" \")[0].alias(\"from_node\").cast(\"int\"),\\\n",
    "                  F.split(answers_sdf.value, \" \")[1].alias(\"to_node\").cast(\"int\"),\\\n",
    "                  F.split(answers_sdf.value, \" \")[2].alias(\"edge_type\").cast(\"string\"))\n",
    "answers_sdf = answers_sdf.withColumn('edge_type', F.lit('answers'))\n",
    "\n",
    "comments_answers_sdf = comments_answers_sdf.select(F.split(comments_answers_sdf.value, \" \")[0].alias(\"from_node\").cast(\"int\"),\\\n",
    "                  F.split(comments_answers_sdf.value, \" \")[1].alias(\"to_node\").cast(\"int\"),\\\n",
    "                  F.split(comments_answers_sdf.value, \" \")[2].alias(\"edge_type\").cast(\"string\"))\n",
    "comments_answers_sdf = comments_answers_sdf.withColumn('edge_type', F.lit('comment-on-answer'))\n",
    "\n",
    "comments_questions_sdf = comments_questions_sdf.select(F.split(comments_questions_sdf.value, \" \")[0].alias(\"from_node\").cast(\"int\"),\\\n",
    "                  F.split(comments_questions_sdf.value, \" \")[1].alias(\"to_node\").cast(\"int\"),\\\n",
    "                  F.split(comments_questions_sdf.value, \" \")[2].alias(\"edge_type\").cast(\"string\"))\n",
    "comments_questions_sdf = comments_questions_sdf.withColumn('edge_type', F.lit('comment-on-question'))\n",
    "\n",
    "answers_sdf.show()\n",
    "comments_answers_sdf.show()\n",
    "comments_questions_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_sdf = answers_sdf.unionAll(comments_answers_sdf).unionAll(comments_questions_sdf)\n",
    "graph_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answers_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_answers_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments_answers_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_answers_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " comments_questions_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_questions_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_questions_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may put any computations you need here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.4.1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: output dataframe with top 10 users by number of questions\n",
    "answers_sdf_count = answers_sdf.groupby(answers_sdf['to_node']).count()\n",
    "\n",
    "answers_sdf_count = answers_sdf_count.createOrReplaceTempView(\"answers_sdf_count_view\")\n",
    "number_of_questions = spark.sql('select * from answers_sdf_count_view order by count DESC')\n",
    "\n",
    "number_of_questions = number_of_questions.createOrReplaceTempView(\"number_of_questions_view\")\n",
    "number_of_questions = spark.sql('select to_node as user, count as ansCounts from number_of_questions_view')\n",
    "\n",
    "number_of_questions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: output top 10 users by number of answers to questions by distinct users\n",
    "answers_sdf_count = answers_sdf.groupby(answers_sdf['from_node']).count()\n",
    "\n",
    "answers_sdf_count = answers_sdf_count.createOrReplaceTempView(\"answers_sdf_count_view\")\n",
    "number_of_answers_to_questions = spark.sql('select * from answers_sdf_count_view order by count DESC')\n",
    "\n",
    "number_of_answers_to_questions = number_of_answers_to_questions.createOrReplaceTempView(\"number_of_answers_to_questions_view\")\n",
    "number_of_answers_to_questions = spark.sql('select from_node as user, count as ansCounts from number_of_answers_to_questions_view')\n",
    "\n",
    "number_of_answers_to_questions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.4.2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: number of users whose questions have never been answered but commented on\n",
    "\n",
    "left = comments_questions_sdf\n",
    "right = answers_sdf\n",
    "\n",
    "combine = left.join(right, left.to_node == right.to_node, how = 'leftanti').drop_duplicates(['to_node'])\n",
    "combine.count()\n",
    "combine.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.4.3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: top 10 pairs of users by mutual answers, along with the number of questions they have mutually answered\n",
    "pairs_sdf = answers_sdf.groupby(answers_sdf['from_node'], answers_sdf['to_node']).count()\n",
    "pairs_sdf.createOrReplaceTempView(\"pairs_sdf_view\")\n",
    "pairs_sdf_ordered = spark.sql('select * from pairs_sdf_view order by count DESC')\n",
    "\n",
    "pairs_sdf_ordered.createOrReplaceTempView(\"pairs_sdf_ordered_view\")\n",
    "pairs_sdf_ordered = spark.sql('\\\n",
    "select * from pairs_sdf_ordered_view \\\n",
    "where pairs_sdf_ordered_view.from_node != pairs_sdf_ordered_view.to_node')\n",
    "# pairs_sdf_ordered.show()\n",
    "\n",
    "pairs_sdf_ordered.createOrReplaceTempView(\"pairs_sdf_ordered_view\")\n",
    "pairs_entire_sdf = spark.sql('\\\n",
    "SELECT po1.from_node AS user1, po1.to_node AS user2, po1.count + po2.count AS ansCounts \\\n",
    "FROM pairs_sdf_ordered_view AS po1 \\\n",
    "INNER JOIN pairs_sdf_ordered_view AS po2 \\\n",
    "ON po1.from_node = po2.to_node AND po1.to_node = po2.from_node \\\n",
    "WHERE po1.from_node < po1.to_node \\\n",
    "ORDER BY ansCounts DESC ')\n",
    "pairs_entire_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in according to HW spec\n",
    "\n",
    "graph_sdf_out_count = graph_sdf.groupby(graph_sdf['from_node']).count()\n",
    "graph_sdf_out_count = graph_sdf_out_count.createOrReplaceTempView(\"graph_sdf_out_count_view\")\n",
    "highest_outdegree_sdf = spark.sql('select * from graph_sdf_out_count_view order by count DESC')\n",
    "\n",
    "graph_sdf_in_count = graph_sdf.groupby(graph_sdf['to_node']).count()\n",
    "graph_sdf_in_count = graph_sdf_in_count.createOrReplaceTempView(\"graph_sdf_in_count_view\")\n",
    "highest_indegree_sdf = spark.sql('select * from graph_sdf_in_count_view order by count DESC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_indegree_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_outdegree_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert code as you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"node\",IntegerType(),True)])\n",
    "schema1 = StructType([StructField(\"node\",IntegerType(),True),StructField(\"depth\",IntegerType(),True)])\n",
    "\n",
    "def spark_bfs(G, origins, max_depth):\n",
    "    \n",
    "    ##Your logic goes here\n",
    "    unexplored_sdf = G.repartition(100,'from_node').cache()\n",
    "    frontier_sdf = spark.createDataFrame(origins, schema).cache()\n",
    "    visited_sdf = spark.createDataFrame(origins, schema1)\n",
    "    visited_sdf = visited_sdf.union(frontier_sdf.withColumn('depth',F.lit(0)))\n",
    "    \n",
    "    for depth in range(max_depth):\n",
    "        \n",
    "        unexplored_sdf = unexplored_sdf.join(frontier_sdf, frontier_sdf.node == unexplored_sdf.to_node,'leftanti').cache()\n",
    "        edge_sdf = unexplored_sdf.join(frontier_sdf, frontier_sdf.node == unexplored_sdf.from_node,'inner')\n",
    "        next_sdf = edge_sdf.select(edge_sdf.to_node.alias('node'))\n",
    "        visited_sdf = visited_sdf.union(next_sdf.withColumn('depth',F.lit(depth + 1)))\n",
    "        frontier_sdf = next_sdf.drop_duplicates().cache()\n",
    "    \n",
    "    unexplored_sdf.unpersist()\n",
    "    frontier_sdf.unpersist()\n",
    "    return visited_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comment out this line once your code is ready\n",
    "bfs_sdf = spark.createDataFrame([{'node': 123, 'depth': 1}, {'node': 456, 'depth': 2}])\n",
    "\n",
    "# TODO: enable this once your code is ready\n",
    "origin_map = [{'node': 4550}, {'node': 242}]\n",
    "bfs_sdf = spark_bfs(comments_questions_sdf, origin_map, 2)\n",
    "bfs_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert code as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs_sdf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Step 4.2 Pre-processing\n",
    "#\n",
    "def friend_rec(input_sdf, graph_sdf):\n",
    "    \n",
    "    ## Your logic goes in here\n",
    "    graph_sdf.cache()\n",
    "    \n",
    "    total_sdf = input_sdf.select('node')\n",
    "    friends_out_sdf = total_sdf.join(graph_sdf, total_sdf.node == graph_sdf.from_node, 'inner')\n",
    "    friends_out_sdf = friends_out_sdf.select(friends_out_sdf.node, friends_out_sdf.to_node.alias('friend'))\n",
    "    \n",
    "    friends_in_sdf = total_sdf.join(graph_sdf, total_sdf.node == graph_sdf.to_node, 'inner')\n",
    "    friends_in_sdf = friends_in_sdf.select(friends_in_sdf.node, friends_in_sdf.from_node.alias('friend'))\n",
    "    \n",
    "    graph_sdf.unpersist()\n",
    "    total_friends_sdf = friends_out_sdf.union(friends_in_sdf).distinct()\n",
    "    total_friends_sdf = total_friends_sdf.repartition(100, 'node').cache()\n",
    "    total_friends_sdf = total_friends_sdf.where(total_friends_sdf.node != total_friends_sdf.friend).cache()\n",
    "    \n",
    "    total_friends_sdf.createOrReplaceTempView('total_friends_view')\n",
    "    friend_rec_sdf = spark.sql('select frec1.node as from_node, frec2.node as to_node, \\\n",
    "    count(frec1.friend) as connections \\\n",
    "    from total_friends_view as frec1 \\\n",
    "    inner join total_friends_view as frec2 \\\n",
    "    on frec1.friend = frec2.friend \\\n",
    "    where frec1.node != frec2.node \\\n",
    "    group by frec1.node, frec2.node').cache()\n",
    "    \n",
    "    friend_rec_sdf = friend_rec_sdf.where(friend_rec_sdf.connections >= 2).select('from_node', 'to_node').cache()\n",
    "    friend_rec_sdf = friend_rec_sdf.join(total_friends_sdf, [total_friends_sdf.node == friend_rec_sdf.from_node, total_friends_sdf.friend == friend_rec_sdf.to_node], 'leftanti').drop_duplicates()\n",
    "    \n",
    "    total_friends_sdf.unpersist()\n",
    "    return friend_rec_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bfs_sdf = bfs_sdf[bfs_sdf.depth == 2].groupBy('node', 'depth').count()\n",
    "# count is a method\n",
    "filtered_bfs_sdf = filtered_bfs_sdf.withColumnRenamed('count', 'num')\n",
    "filtered_bfs_sdf = filtered_bfs_sdf[filtered_bfs_sdf.num > 1]\n",
    "filtered_bfs_sdf = filtered_bfs_sdf.drop('num')\n",
    "filtered_bfs_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: enable this when your function works\n",
    "\n",
    "friend_recommendations_sdf = friend_rec(filtered_bfs_sdf, comments_questions_sdf)\n",
    "friend_recommendations_sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friend_recommendations_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friend_recommendations_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3: Graph visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you have excecuted the cells in Step 4.2 and you have friend_recommendations_sdf, lets create friend_recommendations_df using toPandas(). This creates an in-memory dataFrame that we can use to build the graph. Here we have used ('from_node','to_node') as column names in friend_recommendations_sdf, please change it to what you have used in yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# TODO: create friend_graph NetworkX graph from friend_recommendations_df from friend_recommendations_sdf\n",
    "friend_rec_df = friend_recommendations_sdf.toPandas()\n",
    "friend_graph = nx.from_pandas_dataframe(friend_rec_df, 'from_node','to_node')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Number of nodes (characters) in this graph is\", friend_graph.order()) # number of nodes\n",
    "print (\"Number of edges in this graph is\", len(friend_graph.edges())) # number of edges\n",
    "print (\"Graph diameter is\", nx.diameter(friend_graph)) # maximum eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib inline\n",
    " nx.draw(friend_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sdf = answers_sdf.groupby(['from_node']).count()\n",
    "answers_sdf.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
