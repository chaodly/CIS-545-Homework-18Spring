{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "spark = SparkSession.builder.appName('Advanced-HW2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read lines from the text file\n",
    "pr_sdf = spark.read.load('pr_graph.txt', format=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|from_node|to_node|\n",
      "+---------+-------+\n",
      "|        1|      2|\n",
      "|        1|      3|\n",
      "|        1|      4|\n",
      "|        1|      5|\n",
      "|        2|      3|\n",
      "|        2|      5|\n",
      "|        3|      2|\n",
      "|        4|      5|\n",
      "|        5|      1|\n",
      "|        5|      6|\n",
      "|        5|      7|\n",
      "|        6|      7|\n",
      "|        7|      6|\n",
      "|        7|      2|\n",
      "|        7|      7|\n",
      "|        5|      4|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr_sdf = pr_sdf.select(F.split(pr_sdf.value, ' ')[0].alias('from_node').cast('int'),\\\n",
    "                      F.split(pr_sdf.value, ' ')[1].alias('to_node').cast('int'))\n",
    "pr_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(G, num_iter):\n",
    "    \n",
    "    G.cache()\n",
    "    # user defined\n",
    "    fn_re = F.udf(lambda x :1/x, DoubleType())\n",
    "    \n",
    "    weights_sdf = G.groupBy('from_node').count().withColumnRenamed('from_node', 'node')\n",
    "    weights_sdf = weights_sdf.select('node', fn_re('count').alias('weight'))\n",
    "    \n",
    "    weighted_pr_sdf = G.join(weights_sdf, weights_sdf.node == G.from_node, 'inner')\n",
    "    weighted_pr_sdf = weighted_pr_sdf.drop('node').cache()\n",
    "    weighted_pr_sdf.createOrReplaceTempView('weighted_pr_sdf_view')\n",
    "    \n",
    "    pr_values_sdf = G.select(G.from_node).drop_duplicates().withColumn('pagerank', F.lit(1/ G.count()))\n",
    "    pr_values_sdf = pr_values_sdf.withColumnRenamed('from_node','node')\n",
    "    \n",
    "    G.unpersist()\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        pr_values_sdf.createOrReplaceTempView('pr_values_sdf_view')\n",
    "        pr_values_sdf = spark.sql('select weighted_pr_sdf_view.to_node as node,\\\n",
    "        0.85 * sum(pr_values_sdf_view.pagerank * weighted_pr_sdf_view.weight) + 0.15 as pagerank \\\n",
    "        from pr_values_sdf_view inner join weighted_pr_sdf_view\\\n",
    "        on pr_values_sdf_view.node = weighted_pr_sdf_view.from_node\\\n",
    "        group by weighted_pr_sdf_view.to_node\\\n",
    "        order by weighted_pr_sdf_view.to_node')\n",
    "    \n",
    "    weighted_pr_sdf.unpersist()\n",
    "\n",
    "    return pr_values_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|from_node|to_node|\n",
      "+---------+-------+\n",
      "|        1|      2|\n",
      "|        1|      3|\n",
      "|        1|      4|\n",
      "|        1|      5|\n",
      "|        2|      3|\n",
      "|        2|      5|\n",
      "|        3|      2|\n",
      "|        4|      5|\n",
      "|        5|      1|\n",
      "|        5|      6|\n",
      "|        5|      7|\n",
      "|        6|      7|\n",
      "|        7|      6|\n",
      "|        7|      2|\n",
      "|        7|      7|\n",
      "|        5|      4|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|node|           pagerank|\n",
      "+----+-------------------+\n",
      "|   1|0.29434346987900906|\n",
      "|   2| 0.7901554423964857|\n",
      "|   3|0.49717473280894897|\n",
      "|   4| 0.3519121121540776|\n",
      "|   5| 0.7712117128233261|\n",
      "|   6| 0.5055626720192206|\n",
      "|   7| 0.8778237446376815|\n",
      "+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pagerank(pr_sdf, 5).orderBy(\"node\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
